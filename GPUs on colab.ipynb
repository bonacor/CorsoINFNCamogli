{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is your notebook running on GPU backend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google colab gives you GPU and TPU for-free. \n",
    "* by \"GPU\" I mean 1 core of a [Nvidia Tesla K80](https://www.nvidia.com/it-it/data-center/tesla-k80/) on a *shared* VM (GCP in late 2018 charges 0.45 USD/K80/hr)\n",
    "* by \"TPU\" I mean [Google's Cloud TPU](https://cloud.google.com/tpu) (1st family so far) (GCP in late 2018 charges 4.50 USD/TPU/hr)\n",
    "\n",
    "I.e. using the code below while running on a CPU-only resource is unnecessary, while it may be useful while in colab (or on a GPU you have access to, anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the output above is empty, **you are using a CPU**.\n",
    "\n",
    "If the output above is not empty, **you are connected to a GPU backend**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output above is empty, or gives error, **you are using a CPU**.\n",
    "\n",
    "If the output above is not empty, **you are connected to a GPU backend**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "[if you are using colab]\n",
    "\n",
    "In a nutshell, if you are unlucky, you might see:\n",
    "\n",
    "    Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB\n",
    "    GPU RAM Free: 566MB | Used: 10873MB | Util  95% | Total 11439MB\n",
    "\n",
    "If you are lucky, you will see e.g.:\n",
    "\n",
    "    Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB\n",
    "    GPU RAM Free: 11439MB | Used: 0MB | Util  0% | Total 11439MB\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking more on your CPU, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p tensorflow,numpy -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
